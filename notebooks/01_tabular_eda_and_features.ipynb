{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31ece9f5",
      "metadata": {},
      "source": [
        "# 01 – Tabular EDA & Feature Engineering\n",
        "\n",
        "This notebook focuses on **Exploratory Data Analysis (EDA)** and **feature engineering** for tabular data using the `ml_tabular` template.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Load configuration and data\n",
        "2. Inspect schema, types, and basic statistics\n",
        "3. Analyze missing values and target distribution\n",
        "4. Explore feature–target relationships\n",
        "5. Explore correlations and potential leakage\n",
        "6. Prototype basic feature engineering\n",
        "7. (Optionally) write updated feature config back to YAML\n",
        "\n",
        "The goal is not just to look at pretty plots, but to make **concrete decisions** about:\n",
        "\n",
        "- Which columns to use\n",
        "- Which feature types they are (numeric / categorical / datetime)\n",
        "- What transformations we might want (log transforms, date decomposition, etc.)\n",
        "- How to reflect those decisions in the project config and pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba4e9ed",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "\n",
        "We assume you have installed the project (from the repo root):\n",
        "\n",
        "```bash\n",
        "pip install -e .[dev]\n",
        "```\n",
        "\n",
        "And that you have a tabular config at:\n",
        "\n",
        "- `configs/tabular/train_tabular_baseline.yaml`\n",
        "\n",
        "This notebook will *not* run long training jobs; it focuses on **understanding the data** and **prototyping feature ideas**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8ecd37",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ml_tabular import (\n",
        "    get_config,\n",
        "    get_paths,\n",
        "    get_logger,\n",
        ")\n",
        "\n",
        "LOGGER = get_logger(__name__)\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"tabular\" / \"train_tabular_baseline.yaml\"\n",
        "\n",
        "assert CONFIG_PATH.exists(), f\"Config not found: {CONFIG_PATH}\"\n",
        "\n",
        "cfg = get_config(config_path=CONFIG_PATH, env=\"dev\", force_reload=True)\n",
        "paths = get_paths(config_path=CONFIG_PATH, env=\"dev\", force_reload=True)\n",
        "\n",
        "cfg_dict = cfg.to_dict()\n",
        "tab_cfg = cfg_dict.get(\"tabular\", {})\n",
        "\n",
        "tab_cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d84c0140",
      "metadata": {},
      "source": [
        "## 1. Load data\n",
        "\n",
        "We use the dataset path specified in the tabular config, typically something like:\n",
        "\n",
        "- `data/train.csv`\n",
        "\n",
        "We also validate that the feature and target columns listed in config exist in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e22d4f62",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_csv = tab_cfg[\"dataset_csv\"]\n",
        "data_path = paths.data_dir / dataset_csv\n",
        "assert data_path.exists(), f\"Dataset not found: {data_path}\"\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "LOGGER.info(\"Loaded dataset with shape: %s\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1406c377",
      "metadata": {},
      "source": [
        "Check that config columns exist and log them for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96919ab0",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_columns = tab_cfg.get(\"feature_columns\") or []\n",
        "target_column = tab_cfg.get(\"target_column\")\n",
        "\n",
        "print(\"Configured feature columns (from YAML):\", feature_columns)\n",
        "print(\"Configured target column:\", target_column)\n",
        "print(\"DataFrame columns:\", list(df.columns))\n",
        "\n",
        "missing_features = [c for c in feature_columns if c not in df.columns]\n",
        "missing_target = target_column not in df.columns if target_column is not None else True\n",
        "\n",
        "print(\"Missing features:\", missing_features)\n",
        "print(\"Missing target?\", missing_target)\n",
        "\n",
        "if missing_features:\n",
        "    LOGGER.warning(\"Some configured features are missing from the dataset: %s\", missing_features)\n",
        "if missing_target:\n",
        "    LOGGER.warning(\"Configured target column '%s' is missing from the dataset.\", target_column)\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906ff3ff",
      "metadata": {},
      "source": [
        "## 2. High-level schema and types\n",
        "\n",
        "We start with basic schema checks:\n",
        "\n",
        "- Column names\n",
        "- Dtypes\n",
        "- Number of unique values (for categorical-ish columns)\n",
        "- Basic `describe()` statistics\n",
        "\n",
        "This helps us decide which columns should be treated as **numeric**, **categorical**, or **datetime**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc5bede",
      "metadata": {
        "tags": [
          "schema"
        ]
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9744b718",
      "metadata": {
        "tags": [
          "describe"
        ]
      },
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "non_numeric_cols = [c for c in df.columns if c not in numeric_cols]\n",
        "\n",
        "print(\"Numeric columns:\", numeric_cols)\n",
        "print(\"Non-numeric columns:\", non_numeric_cols)\n",
        "\n",
        "df[numeric_cols].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1545f8",
      "metadata": {
        "tags": [
          "uniques"
        ]
      },
      "outputs": [],
      "source": [
        "unique_counts = df[non_numeric_cols].nunique(dropna=True).sort_values()\n",
        "unique_counts.to_frame(\"n_unique\").T if unique_counts.empty else unique_counts.to_frame(\"n_unique\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7ed74b",
      "metadata": {},
      "source": [
        "> **Guideline:**\n",
        "> - Low-cardinality non-numeric columns (e.g. `n_unique < 50`) often make good candidates for **categorical** features.\n",
        "> - Columns that *look* like dates (e.g. strings like `2023-01-01`) should typically be parsed to `datetime64` and treated as **datetime**.\n",
        "\n",
        "We can attempt to automatically parse obvious datetime columns next (purely for exploration; the proper handling can be wired into your config and pipelines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "384fa7c5",
      "metadata": {
        "tags": [
          "datetime"
        ]
      },
      "outputs": [],
      "source": [
        "def try_parse_dates(df: pd.DataFrame, max_cols: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"Attempt to parse object columns as datetimes (for EDA only).\n",
        "\n",
        "    We only try the first `max_cols` object columns to avoid surprises on huge datasets.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    obj_cols = df_copy.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()[:max_cols]\n",
        "    for col in obj_cols:\n",
        "        try:\n",
        "            parsed = pd.to_datetime(df_copy[col], errors=\"raise\")\n",
        "        except Exception:\n",
        "            continue\n",
        "        else:\n",
        "            LOGGER.info(\"Parsed column '%s' as datetime for EDA\", col)\n",
        "            df_copy[col] = parsed\n",
        "    return df_copy\n",
        "\n",
        "df_eda = try_parse_dates(df)\n",
        "df_eda.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52384b62",
      "metadata": {},
      "source": [
        "## 3. Missing values analysis\n",
        "\n",
        "Understanding missingness is crucial both for **feature engineering** and **data quality**.\n",
        "\n",
        "We’ll compute:\n",
        "- Count and percentage of missing values per column\n",
        "- Simple bar plot of missingness (if any)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7320547",
      "metadata": {
        "tags": [
          "missing"
        ]
      },
      "outputs": [],
      "source": [
        "missing_counts = df_eda.isna().sum()\n",
        "missing_pct = (missing_counts / len(df_eda)) * 100.0\n",
        "\n",
        "missing_df = (\n",
        "    pd.DataFrame({\"missing_count\": missing_counts, \"missing_pct\": missing_pct})\n",
        "    .sort_values(\"missing_pct\", ascending=False)\n",
        ")\n",
        "\n",
        "missing_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10855a9",
      "metadata": {
        "tags": [
          "missing_plot"
        ]
      },
      "outputs": [],
      "source": [
        "high_missing = missing_df[missing_df[\"missing_pct\"] > 0]\n",
        "if not high_missing.empty:\n",
        "    plt.figure(figsize=(8, max(3, len(high_missing) * 0.3)))\n",
        "    plt.barh(high_missing.index, high_missing[\"missing_pct\"])\n",
        "    plt.xlabel(\"Missing %\")\n",
        "    plt.title(\"Missing values per column\")\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No missing values detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5451c1de",
      "metadata": {},
      "source": [
        "> **Questions to consider:**\n",
        "> - Which columns have high missingness (e.g. > 30%) and might not be worth keeping?\n",
        "> - For the remaining ones, do we prefer **imputation**, **special categories** (for categoricals), or domain-specific logic?\n",
        "\n",
        "Your answers should inform both your **feature configuration** and how you design your **sklearn/PyTorch pipelines** downstream."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350c2afb",
      "metadata": {},
      "source": [
        "## 4. Target distribution and basic feature–target relationship\n",
        "\n",
        "We analyze the target differently depending on the **task type**:\n",
        "\n",
        "- **Regression**: histogram + summary stats\n",
        "- **Binary classification**: class balance\n",
        "- **Multiclass classification**: class counts\n",
        "\n",
        "Then we’ll look at a few simple feature–target relationships (e.g., numeric features vs target)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb3e2a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "task_type = tab_cfg.get(\"task_type\", \"binary\")\n",
        "target_column = tab_cfg.get(\"target_column\")\n",
        "\n",
        "if target_column is None or target_column not in df_eda.columns:\n",
        "    print(\"Target column not found; skipping target distribution analysis.\")\n",
        "else:\n",
        "    y = df_eda[target_column]\n",
        "    print(\"Target dtype:\", y.dtype)\n",
        "    print(\"Unique values (head):\", y.unique()[:20])\n",
        "\n",
        "    if task_type == \"regression\":\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.hist(y.dropna(), bins=30, edgecolor=\"black\", alpha=0.7)\n",
        "        plt.xlabel(target_column)\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.title(\"Target distribution (regression)\")\n",
        "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        display(y.describe())\n",
        "\n",
        "    else:\n",
        "        value_counts = y.value_counts(dropna=False)\n",
        "        display(value_counts.to_frame(\"count\"))\n",
        "\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(value_counts.index.astype(str), value_counts.values)\n",
        "        plt.xlabel(target_column)\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.title(f\"Target distribution ({task_type})\")\n",
        "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff1c6cc",
      "metadata": {},
      "source": [
        "### 4.1 Simple numeric feature vs target views\n",
        "\n",
        "We can look at a few numeric features to get an intuition of how they relate to the target.\n",
        "\n",
        "- For regression, scatter plots\n",
        "- For classification, box plots per class (or simple groupby means)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3745b95",
      "metadata": {
        "tags": [
          "feature_target"
        ]
      },
      "outputs": [],
      "source": [
        "if target_column is None or target_column not in df_eda.columns:\n",
        "    print(\"Skipping numeric feature vs target analysis; target not available.\")\n",
        "else:\n",
        "    numeric_cols_no_target = [c for c in numeric_cols if c != target_column]\n",
        "    cols_to_plot = numeric_cols_no_target[:4]\n",
        "\n",
        "    if not cols_to_plot:\n",
        "        print(\"No numeric features (other than target) found.\")\n",
        "    else:\n",
        "        n_cols = len(cols_to_plot)\n",
        "        plt.figure(figsize=(4 * n_cols, 4))\n",
        "        for i, col in enumerate(cols_to_plot, start=1):\n",
        "            plt.subplot(1, n_cols, i)\n",
        "            if task_type == \"regression\":\n",
        "                plt.scatter(df_eda[col], df_eda[target_column], alpha=0.5)\n",
        "                plt.xlabel(col)\n",
        "                plt.ylabel(target_column)\n",
        "            else:\n",
        "                # For classification, plot per-class means as a quick heuristic\n",
        "                means = df_eda.groupby(target_column)[col].mean()\n",
        "                plt.bar(means.index.astype(str), means.values)\n",
        "                plt.xlabel(target_column)\n",
        "                plt.ylabel(f\"mean({col})\")\n",
        "            plt.title(col)\n",
        "            plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97bc452",
      "metadata": {},
      "source": [
        "## 5. Correlation and leakage checks\n",
        "\n",
        "We take a quick look at:\n",
        "\n",
        "- Numeric feature correlation matrix\n",
        "- Correlation between each feature and the target (for regression-ish targets)\n",
        "\n",
        "This can reveal:\n",
        "- Highly correlated features (candidates for removal or regularization)\n",
        "- Obvious leakage (e.g., feature almost perfectly correlated with target)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a0fafc",
      "metadata": {
        "tags": [
          "correlation"
        ]
      },
      "outputs": [],
      "source": [
        "if numeric_cols:\n",
        "    corr = df_eda[numeric_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    im = plt.imshow(corr, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=90)\n",
        "    plt.yticks(range(len(numeric_cols)), numeric_cols)\n",
        "    plt.title(\"Correlation matrix (numeric features)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric columns found for correlation matrix.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6791d5",
      "metadata": {
        "tags": [
          "feature_target_corr"
        ]
      },
      "outputs": [],
      "source": [
        "if target_column is not None and target_column in numeric_cols:\n",
        "    # Compute absolute correlation of each feature with the numeric target\n",
        "    target_corr = (\n",
        "        df_eda[numeric_cols]\n",
        "        .corr()[target_column]\n",
        "        .drop(target_column)\n",
        "        .abs()\n",
        "        .sort_values(ascending=False)\n",
        "    )\n",
        "    display(target_corr.to_frame(\"|corr_with_target|\").head(20))\n",
        "else:\n",
        "    print(\"Target is not numeric (or missing); skipping numeric correlation with target.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6452fa04",
      "metadata": {},
      "source": [
        "> **Actionable outcomes:**\n",
        "> - Identify feature pairs with very high correlation (e.g. > 0.95); consider dropping one, or using a model robust to multicollinearity.\n",
        "> - If a feature is almost perfectly correlated with the target, investigate whether it is **data leakage** (e.g. post-outcome information)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef4e515",
      "metadata": {},
      "source": [
        "## 6. Prototype feature engineering\n",
        "\n",
        "This section is for **trying out ideas** like:\n",
        "\n",
        "- Date decomposition (year, month, day, dayofweek, etc.)\n",
        "- Simple numeric transforms (log of skewed variables, interactions)\n",
        "- Simple categorical encodings for EDA (frequency encoding, etc.)\n",
        "\n",
        "The final *production* feature logic should live in your Python modules (e.g. `ml_tabular.features.build_features`) and be configured via YAML, but the notebook is a good place to experiment first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e6bb67",
      "metadata": {
        "tags": [
          "feature_engineering"
        ]
      },
      "outputs": [],
      "source": [
        "# Example: Identify datetime columns we parsed earlier\n",
        "datetime_cols = df_eda.select_dtypes(include=[\"datetime64[ns]\"]).columns.tolist()\n",
        "datetime_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a56a42",
      "metadata": {
        "tags": [
          "date_parts"
        ]
      },
      "outputs": [],
      "source": [
        "def add_date_parts(df: pd.DataFrame, date_cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Add basic date-derived features for given datetime columns.\n",
        "\n",
        "    This is for exploration; in the template, you could move equivalent\n",
        "    logic into `ml_tabular.features.build_features`.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for col in date_cols:\n",
        "        if col not in df_copy.columns:\n",
        "            continue\n",
        "        if not np.issubdtype(df_copy[col].dtype, np.datetime64):\n",
        "            LOGGER.warning(\"Column '%s' is not datetime; skipping date parts.\", col)\n",
        "            continue\n",
        "        base = col\n",
        "        df_copy[f\"{base}_year\"] = df_copy[col].dt.year\n",
        "        df_copy[f\"{base}_month\"] = df_copy[col].dt.month\n",
        "        df_copy[f\"{base}_day\"] = df_copy[col].dt.day\n",
        "        df_copy[f\"{base}_dow\"] = df_copy[col].dt.dayofweek\n",
        "        df_copy[f\"{base}_hour\"] = df_copy[col].dt.hour\n",
        "    return df_copy\n",
        "\n",
        "df_fe = add_date_parts(df_eda, datetime_cols)\n",
        "df_fe.filter(regex=\"_year$|_month$|_dow$|_hour$\").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bbee96",
      "metadata": {
        "tags": [
          "numeric_transforms"
        ]
      },
      "outputs": [],
      "source": [
        "# Example: Identify skewed numeric features and log-transform them (for EDA)\n",
        "numeric_cols_no_target = [c for c in numeric_cols if c != target_column]\n",
        "skew = df_fe[numeric_cols_no_target].skew().sort_values(ascending=False)\n",
        "skew.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a5e8c9",
      "metadata": {
        "tags": [
          "log_skewed"
        ]
      },
      "outputs": [],
      "source": [
        "skew_threshold = 1.0\n",
        "skewed_features = skew[skew.abs() > skew_threshold].index.tolist()\n",
        "print(\"Skewed numeric features (|skew| >\", skew_threshold, \"):\", skewed_features)\n",
        "\n",
        "df_fe_log = df_fe.copy()\n",
        "for col in skewed_features:\n",
        "    # Avoid issues with non-positive values; simple example for EDA\n",
        "    col_min = df_fe_log[col].min()\n",
        "    shifted = df_fe_log[col] - col_min + 1e-6\n",
        "    df_fe_log[col + \"_log1p\"] = np.log1p(shifted)\n",
        "\n",
        "df_fe_log.filter(regex=\"_log1p$\").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56bc3765",
      "metadata": {},
      "source": [
        "You can continue experimenting here:\n",
        "\n",
        "- Numeric interactions (e.g. `feature1 * feature2`, ratios)\n",
        "- Frequency encoding for categoricals\n",
        "- Binning continuous variables\n",
        "\n",
        "When you find transformations that make sense and are *defensible*, you can move them into your shared feature module so they are **reproducible** for training and inference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0554ca24",
      "metadata": {},
      "source": [
        "## 7. Reflect decisions into configuration\n",
        "\n",
        "After EDA, you will often decide:\n",
        "\n",
        "- Which columns to include / exclude\n",
        "- Which columns should be treated as numeric / categorical / datetime\n",
        "- Which engineered features to keep (e.g. date parts, log-transformed features)\n",
        "\n",
        "You have two main options for recording these decisions:\n",
        "\n",
        "1. **Update YAML config**: e.g. add lists like `numeric_features`, `categorical_features`, `datetime_features`, `derived_features` under `tabular`.\n",
        "2. **Update Python feature module**: encode the logic in `ml_tabular.features.build_features` and have the config tell it *what* to do.\n",
        "\n",
        "Below is a small helper to *export* a candidate feature configuration based on what we just inferred/created. You can then manually review and merge it into your main YAML config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fea140",
      "metadata": {
        "tags": [
          "export_config"
        ]
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# Example: propose feature lists\n",
        "\n",
        "candidate_numeric = [c for c in numeric_cols_no_target if c in df_fe_log.columns]\n",
        "candidate_categorical = [\n",
        "    c for c in df_fe_log.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns\n",
        "    if c != target_column\n",
        "]\n",
        "candidate_datetime = datetime_cols\n",
        "candidate_derived = [c for c in df_fe_log.columns if c.endswith(\"_log1p\") or c.endswith(\"_year\") or c.endswith(\"_month\") or c.endswith(\"_dow\") or c.endswith(\"_hour\")]\n",
        "\n",
        "feature_config = {\n",
        "    \"numeric_features\": candidate_numeric,\n",
        "    \"categorical_features\": candidate_categorical,\n",
        "    \"datetime_features\": candidate_datetime,\n",
        "    \"derived_features\": candidate_derived,\n",
        "}\n",
        "\n",
        "feature_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1367ada",
      "metadata": {
        "tags": [
          "write_yaml"
        ]
      },
      "outputs": [],
      "source": [
        "output_config_path = PROJECT_ROOT / \"configs\" / \"tabular\" / \"tabular_features_candidate.yaml\"\n",
        "\n",
        "output_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with output_config_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(feature_config, f, sort_keys=False)\n",
        "\n",
        "LOGGER.info(\"Wrote candidate feature config to: %s\", output_config_path)\n",
        "output_config_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d84710a",
      "metadata": {},
      "source": [
        "## 8. Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "- Loaded config and data through the **same mechanisms** your training code uses\n",
        "- Inspected schema, types, missing values, and target distribution\n",
        "- Explored basic feature–target relationships and correlations\n",
        "- Prototyped reasonable feature engineering ideas (date parts, log transforms)\n",
        "- Exported a **candidate feature configuration** that you can merge into your main YAML\n",
        "\n",
        "This demonstrates that you treat **EDA as part of an engineering process**, not just ad-hoc experimentation: insights here feed directly into your config, pipelines, and final models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
