{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb6d13c1",
      "metadata": {},
      "source": [
        "# 10 – Time Series Quickstart: ml_tabular Template\n",
        "\n",
        "This notebook shows how to use the **ml_tabular** template for a simple time-series forecasting task.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Load configuration from a YAML file (time-series baseline)\n",
        "2. Load and inspect the raw time-series data\n",
        "3. Build a `TimeSeriesSequenceDataset` and PyTorch `DataLoader`s\n",
        "4. Define a small GRU-based forecasting model\n",
        "5. Train and evaluate using shared training utilities (`fit`, `evaluate`, `EarlyStopping`)\n",
        "6. (Optionally) log runs and artifacts to MLflow\n",
        "\n",
        "The aim is to demonstrate the **end-to-end path** for time-series using the same engineering principles as the tabular pipeline: config-driven, testable, and reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db06029",
      "metadata": {},
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "We assume you have:\n",
        "\n",
        "- Installed the project with extras:\n",
        "\n",
        "  ```bash\n",
        "  pip install -e .[dev,mlops]\n",
        "  ```\n",
        "\n",
        "- A baseline config at:\n",
        "\n",
        "  - `configs/time_series/train_ts_baseline.yaml`\n",
        "\n",
        "That config should define at least:\n",
        "\n",
        "- `paths` (data, models)\n",
        "- `training` (batch size, epochs, lr, etc.)\n",
        "- `time_series` (dataset CSV, id column, time column, target, features, lookback, horizon, etc.)\n",
        "\n",
        "This notebook is designed to be run from the **project root** (so relative paths work)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28629c8c",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from ml_tabular import (\n",
        "    get_config,\n",
        "    get_paths,\n",
        "    get_logger,\n",
        "    TimeSeriesSequenceDataset,\n",
        "    train_one_epoch,\n",
        "    evaluate,\n",
        "    fit,\n",
        "    EarlyStopping,\n",
        ")\n",
        "\n",
        "LOGGER = get_logger(__name__)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LOGGER.info(\"Using device: %s\", DEVICE)\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"time_series\" / \"train_ts_baseline.yaml\"\n",
        "assert CONFIG_PATH.exists(), f\"Config not found: {CONFIG_PATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2ddd6f",
      "metadata": {},
      "source": [
        "## 1. Load configuration and inspect time-series settings\n",
        "\n",
        "We use the same `AppConfig` / `get_config` pattern as in the tabular quickstart, but now read the **time-series** section of the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc47555",
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = get_config(config_path=CONFIG_PATH, env=\"dev\", force_reload=True)\n",
        "paths = get_paths(config_path=CONFIG_PATH, env=\"dev\", force_reload=True)\n",
        "\n",
        "cfg_dict = cfg.to_dict()\n",
        "ts_cfg = cfg_dict.get(\"time_series\", {})\n",
        "ts_cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158e4a79",
      "metadata": {},
      "source": [
        "We expect `time_series` config to look roughly like:\n",
        "\n",
        "```yaml\n",
        "time_series:\n",
        "  dataset_csv: \"timeseries.csv\"\n",
        "  id_column: \"series_id\"           # or null for single series\n",
        "  time_column: \"timestamp\"\n",
        "  target_column: \"y\"\n",
        "  feature_columns: [\"x1\", \"x2\", ...]\n",
        "  lookback: 24                      # input sequence length\n",
        "  horizon: 1                        # forecast horizon (e.g. predict next step)\n",
        "  val_fraction: 0.2                 # optional; or explicit split timestamp\n",
        "```\n",
        "\n",
        "You can adapt field names, but the notebook assumes these semantics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52e7543",
      "metadata": {},
      "source": [
        "## 2. Load and inspect the dataset\n",
        "\n",
        "We read the configured CSV and perform some basic sanity checks:\n",
        "\n",
        "- Columns exist\n",
        "- Time column can be parsed as datetime\n",
        "- Data is sorted by (id, time) for sequence generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "391584f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_csv = ts_cfg[\"dataset_csv\"]\n",
        "data_path = paths.data_dir / dataset_csv\n",
        "assert data_path.exists(), f\"Dataset not found: {data_path}\"\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "LOGGER.info(\"Loaded dataset with shape: %s\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1938e41e",
      "metadata": {
        "tags": [
          "columns_check"
        ]
      },
      "outputs": [],
      "source": [
        "id_col = ts_cfg.get(\"id_column\")  # may be None for single series\n",
        "time_col = ts_cfg[\"time_column\"]\n",
        "target_col = ts_cfg[\"target_column\"]\n",
        "feature_cols = ts_cfg.get(\"feature_columns\") or []\n",
        "\n",
        "print(\"ID column:\", id_col)\n",
        "print(\"Time column:\", time_col)\n",
        "print(\"Target column:\", target_col)\n",
        "print(\"Feature columns:\", feature_cols)\n",
        "\n",
        "missing = [c for c in [time_col, target_col] + feature_cols if c not in df.columns]\n",
        "print(\"Missing columns:\", missing)\n",
        "assert not missing, f\"Config references columns not found in dataset: {missing}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f28d92ca",
      "metadata": {},
      "source": [
        "Parse the time column and sort by id/time (or just time for single-series data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d7c21a",
      "metadata": {
        "tags": [
          "parse_time"
        ]
      },
      "outputs": [],
      "source": [
        "df[time_col] = pd.to_datetime(df[time_col], errors=\"raise\")\n",
        "\n",
        "if id_col is not None:\n",
        "    df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
        "else:\n",
        "    df = df.sort_values(time_col).reset_index(drop=True)\n",
        "\n",
        "df[[c for c in [id_col, time_col, target_col] if c is not None]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25770f05",
      "metadata": {},
      "source": [
        "We can also quickly inspect how many unique series we have (if `id_column` is set) and the time coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a993fe6d",
      "metadata": {
        "tags": [
          "coverage"
        ]
      },
      "outputs": [],
      "source": [
        "if id_col is not None:\n",
        "    print(\"Number of series:\", df[id_col].nunique())\n",
        "    display(df.groupby(id_col)[time_col].agg([\"min\", \"max\", \"count\"]).head())\n",
        "else:\n",
        "    print(\"Single series dataset.\")\n",
        "    print(\"Time range:\", df[time_col].min(), \"->\", df[time_col].max())\n",
        "    print(\"Number of points:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d4de502",
      "metadata": {},
      "source": [
        "## 3. Train/validation split by time\n",
        "\n",
        "For time series, we **do not shuffle** in time; instead we split chronologically.\n",
        "\n",
        "We use a simple approach:\n",
        "\n",
        "- Choose a `val_fraction` (e.g. 0.2)\n",
        "- For each series (or globally for single-series data), use the first `(1 - val_fraction)` fraction for training and the rest for validation.\n",
        "\n",
        "For this quickstart, we’ll apply a **global time split** (suitable for many scenarios). You can extend your pipeline for more sophisticated splitting if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "082321b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "val_fraction = float(ts_cfg.get(\"val_fraction\", 0.2))\n",
        "assert 0.0 < val_fraction < 1.0, \"val_fraction should be in (0, 1).\"\n",
        "\n",
        "n_total = len(df)\n",
        "n_train = int((1.0 - val_fraction) * n_total)\n",
        "\n",
        "train_df = df.iloc[:n_train].copy()\n",
        "val_df = df.iloc[n_train:].copy()\n",
        "\n",
        "LOGGER.info(\"Train size: %d, Val size: %d\", len(train_df), len(val_df))\n",
        "train_df[[c for c in [id_col, time_col, target_col] if c is not None]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0b080d",
      "metadata": {},
      "source": [
        "## 4. Build `TimeSeriesSequenceDataset` and DataLoaders\n",
        "\n",
        "We now turn the time-indexed dataframe into sequences for supervised learning. The semantics we assume for `TimeSeriesSequenceDataset`:\n",
        "\n",
        "- Each sample is `(X_seq, y_target)` where:\n",
        "  - `X_seq` has shape `(lookback, n_features)`\n",
        "  - `y_target` has shape `(horizon,)` or scalar for horizon=1\n",
        "- Sequences slide along time for each series independently (if `id_column` is set).\n",
        "\n",
        "We’ll use the `lookback` and `horizon` from config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0185b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "lookback = int(ts_cfg.get(\"lookback\", 24))\n",
        "horizon = int(ts_cfg.get(\"horizon\", 1))\n",
        "\n",
        "print(\"Lookback:\", lookback)\n",
        "print(\"Horizon:\", horizon)\n",
        "\n",
        "train_ds = TimeSeriesSequenceDataset.from_dataframe(\n",
        "    train_df,\n",
        "    id_column=id_col,\n",
        "    time_column=time_col,\n",
        "    feature_columns=feature_cols,\n",
        "    target_column=target_col,\n",
        "    lookback=lookback,\n",
        "    horizon=horizon,\n",
        ")\n",
        "\n",
        "val_ds = TimeSeriesSequenceDataset.from_dataframe(\n",
        "    val_df,\n",
        "    id_column=id_col,\n",
        "    time_column=time_col,\n",
        "    feature_columns=feature_cols,\n",
        "    target_column=target_col,\n",
        "    lookback=lookback,\n",
        "    horizon=horizon,\n",
        ")\n",
        "\n",
        "print(\"Train sequences:\", len(train_ds))\n",
        "print(\"Val sequences:\", len(val_ds))\n",
        "print(\"Metadata (train):\", train_ds.metadata)\n",
        "\n",
        "x_seq, y_target = train_ds[0]\n",
        "print(\"X_seq shape:\", x_seq.shape)\n",
        "print(\"y_target shape:\", y_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90972e16",
      "metadata": {
        "tags": [
          "dataloaders"
        ]
      },
      "outputs": [],
      "source": [
        "batch_size = cfg.training.batch_size if hasattr(cfg, \"training\") else 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "batch_x, batch_y = next(iter(train_loader))\n",
        "print(\"Batch X shape (batch, seq_len, features):\", batch_x.shape)\n",
        "print(\"Batch y shape:\", batch_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131b6eb8",
      "metadata": {},
      "source": [
        "## 5. Define a simple GRU-based forecasting model\n",
        "\n",
        "For this quickstart, we build a small **GRU forecaster** inline in the notebook:\n",
        "\n",
        "- Input: `(batch_size, seq_len, n_features)`\n",
        "- GRU layers process the sequence\n",
        "- We take the **last hidden state** and feed it through a linear layer to produce a forecast vector of size `horizon`.\n",
        "\n",
        "This demonstrates how your template happily supports deep learning models with sequence structure, not just tabular MLPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b021c60",
      "metadata": {
        "tags": [
          "model"
        ]
      },
      "outputs": [],
      "source": [
        "class GRUForecaster(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int = 64,\n",
        "        num_layers: int = 1,\n",
        "        horizon: int = 1,\n",
        "        dropout: float = 0.0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, horizon)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x:\n",
        "            Tensor of shape (batch, seq_len, input_dim).\n",
        "        \"\"\"\n",
        "        out, h_n = self.gru(x)\n",
        "        # h_n: (num_layers, batch, hidden_dim) -> use last layer's hidden state\n",
        "        last_hidden = h_n[-1]  # (batch, hidden_dim)\n",
        "        preds = self.head(last_hidden)  # (batch, horizon)\n",
        "        return preds\n",
        "\n",
        "\n",
        "n_features = train_ds.metadata.get(\"n_features\", len(feature_cols))\n",
        "hidden_dim = int(ts_cfg.get(\"hidden_dim\", 64))\n",
        "num_layers = int(ts_cfg.get(\"num_layers\", 1))\n",
        "dropout = float(ts_cfg.get(\"dropout\", 0.0))\n",
        "\n",
        "model = GRUForecaster(\n",
        "    input_dim=n_features,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    horizon=horizon,\n",
        "    dropout=dropout,\n",
        ").to(DEVICE)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81cf1d0a",
      "metadata": {},
      "source": [
        "## 6. Train and evaluate using shared utilities\n",
        "\n",
        "We now tie everything together:\n",
        "\n",
        "- Use `MSELoss` for regression-style forecasting\n",
        "- Use Adam optimizer with hyperparameters from config\n",
        "- Optionally use `EarlyStopping` based on validation loss\n",
        "- Call `fit(...)` to run the training loop\n",
        "\n",
        "This is exactly the same training infrastructure (`fit`, `EarlyStopping`) as the tabular pipeline, just with a sequence model and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290656ca",
      "metadata": {
        "tags": [
          "training"
        ]
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "learning_rate = cfg.training.learning_rate if hasattr(cfg, \"training\") else 1e-3\n",
        "weight_decay = getattr(cfg.training, \"weight_decay\", 0.0) if hasattr(cfg, \"training\") else 0.0\n",
        "num_epochs = cfg.training.num_epochs if hasattr(cfg, \"training\") else 10\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Optional early stopping config\n",
        "es_cfg = cfg_dict.get(\"training\", {}).get(\"early_stopping\", {})\n",
        "if es_cfg:\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=int(es_cfg.get(\"patience\", 5)),\n",
        "        min_delta=float(es_cfg.get(\"min_delta\", 1e-4)),\n",
        "        mode=es_cfg.get(\"mode\", \"min\"),\n",
        "    )\n",
        "else:\n",
        "    early_stopping = None\n",
        "\n",
        "history = fit(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=num_epochs,\n",
        "    device=DEVICE,\n",
        "    early_stopping=early_stopping,\n",
        ")\n",
        "\n",
        "history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815e71d2",
      "metadata": {},
      "source": [
        "Plot training vs validation loss to verify convergence and overfitting behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb45efa",
      "metadata": {
        "tags": [
          "plot"
        ]
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = history[\"train_losses\"]\n",
        "val_losses = history[\"val_losses\"]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_losses, marker=\"o\", label=\"train\")\n",
        "plt.plot(val_losses, marker=\"o\", label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.title(\"Time-series training vs validation loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9a671c",
      "metadata": {},
      "source": [
        "## 7. Inspect predictions vs ground truth\n",
        "\n",
        "It’s useful to look at a few sequences and compare predicted vs actual targets. For horizon=1, this is just comparing next-step forecasts; for horizon>1, we can plot the forecast window vs true future values.\n",
        "\n",
        "The quick visualization below assumes a univariate target with horizon=1 or a small horizon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a12563fa",
      "metadata": {
        "tags": [
          "predictions"
        ]
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch_x_val, batch_y_val = next(iter(val_loader))\n",
        "    batch_x_val = batch_x_val.to(DEVICE)\n",
        "    preds = model(batch_x_val).cpu().numpy()\n",
        "    y_true = batch_y_val.numpy()\n",
        "\n",
        "print(\"Preds shape:\", preds.shape)\n",
        "print(\"True shape:\", y_true.shape)\n",
        "\n",
        "# Take first few samples to visualize\n",
        "n_plot = min(10, preds.shape[0])\n",
        "t = np.arange(n_plot)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "if horizon == 1:\n",
        "    plt.plot(t, y_true[:n_plot, 0], marker=\"o\", label=\"true\")\n",
        "    plt.plot(t, preds[:n_plot, 0], marker=\"x\", label=\"pred\")\n",
        "    plt.xlabel(\"Sample index (in validation batch)\")\n",
        "    plt.ylabel(\"Target\")\n",
        "    plt.title(\"Next-step forecast: true vs pred\")\n",
        "else:\n",
        "    # For multi-step horizon, just compare first horizon element for each sample\n",
        "    plt.plot(t, y_true[:n_plot, 0], marker=\"o\", label=\"true (step 1)\")\n",
        "    plt.plot(t, preds[:n_plot, 0], marker=\"x\", label=\"pred (step 1)\")\n",
        "    plt.xlabel(\"Sample index (in validation batch)\")\n",
        "    plt.ylabel(\"Target (first horizon step)\")\n",
        "    plt.title(\"Multi-step forecast (first horizon step)\")\n",
        "\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fe95c5",
      "metadata": {},
      "source": [
        "## 8. Save model artifacts\n",
        "\n",
        "We now save the trained model to the `models` directory defined in the config, along with enough metadata to reproduce or reload the run later.\n",
        "\n",
        "This mirrors what your time-series training script (`train_ts_mlp.py` or similar) would do in a non-notebook context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e65801",
      "metadata": {
        "tags": [
          "save"
        ]
      },
      "outputs": [],
      "source": [
        "paths.models_dir.mkdir(parents=True, exist_ok=True)\n",
        "model_path = paths.models_dir / f\"ts_gru_{cfg.experiment_name}.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"model_hparams\": {\n",
        "        \"input_dim\": n_features,\n",
        "        \"hidden_dim\": hidden_dim,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"horizon\": horizon,\n",
        "        \"lookback\": lookback,\n",
        "    },\n",
        "    \"config\": cfg_dict,\n",
        "}, model_path)\n",
        "\n",
        "LOGGER.info(\"Saved time-series model to: %s\", model_path)\n",
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da060754",
      "metadata": {},
      "source": [
        "## 9. (Optional) Enable MLflow tracking\n",
        "\n",
        "If you installed the `mlops` extra and configured MLflow, you can track this training run using `ml_tabular.mlops.mlflow_utils`.\n",
        "\n",
        "This section is **optional**; comment it out if you don’t have MLflow set up yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673f8bb1",
      "metadata": {
        "tags": [
          "mlflow",
          "optional"
        ]
      },
      "outputs": [],
      "source": [
        "from ml_tabular.mlops.mlflow_utils import (\n",
        "    is_mlflow_available,\n",
        "    mlflow_run,\n",
        "    log_params,\n",
        "    log_metrics,\n",
        "    log_artifact,\n",
        ")\n",
        "import os\n",
        "\n",
        "if is_mlflow_available():\n",
        "    import mlflow\n",
        "\n",
        "    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\") or (paths.base_dir / \"mlruns\").as_uri()\n",
        "    experiment_name = cfg.experiment_name + \"_ts\"\n",
        "\n",
        "    with mlflow_run(\n",
        "        enabled=True,\n",
        "        experiment_name=experiment_name,\n",
        "        run_name=\"time_series_quickstart\",\n",
        "        tracking_uri=tracking_uri,\n",
        "        tags={\"template\": \"ml_tabular\", \"notebook\": \"10_time_series_quickstart\"},\n",
        "    ):\n",
        "        # Log run-level parameters\n",
        "        log_params({\n",
        "            \"model_type\": \"GRUForecaster\",\n",
        "            \"input_dim\": n_features,\n",
        "            \"hidden_dim\": hidden_dim,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"horizon\": horizon,\n",
        "            \"lookback\": lookback,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"num_epochs\": num_epochs,\n",
        "        })\n",
        "\n",
        "        # Optionally re-run a short training (or reuse history from above)\n",
        "        # Here we reuse the already-trained model and history\n",
        "        log_metrics({\n",
        "            \"final_train_loss\": float(history[\"train_losses\"][-1]),\n",
        "            \"final_val_loss\": float(history[\"val_losses\"][-1]),\n",
        "        })\n",
        "\n",
        "        if model_path.exists():\n",
        "            log_artifact(model_path, artifact_path=\"models\")\n",
        "else:\n",
        "    print(\"MLflow not available; skipping MLflow tracking demo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ea73a7",
      "metadata": {},
      "source": [
        "## 10. Summary\n",
        "\n",
        "In this time-series quickstart, you:\n",
        "\n",
        "- Loaded configuration via the same `AppConfig` system used for tabular\n",
        "- Parsed and sorted time-indexed data, with optional series IDs\n",
        "- Converted the dataset into supervised sequences using `TimeSeriesSequenceDataset`\n",
        "- Built a small GRU-based model for forecasting\n",
        "- Trained with shared utilities (`fit`, `EarlyStopping`) and visualized loss curves\n",
        "- Saved model artifacts and (optionally) logged them to MLflow\n",
        "\n",
        "This reinforces the core point of your template: **a consistent, professional workflow** across modalities (tabular and time-series), with clean separation between:\n",
        "\n",
        "- Config\n",
        "- Data & datasets\n",
        "- Models\n",
        "- Training loops\n",
        "- MLOps / experiment tracking\n",
        "\n",
        "From here, you can swap in more advanced models (Temporal CNNs, Transformers, etc.) without changing the surrounding plumbing."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
